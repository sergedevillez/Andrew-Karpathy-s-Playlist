{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d92ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  2925k      0 --:--:-- --:--:-- --:--:-- 2951k\n"
     ]
    }
   ],
   "source": [
    "# Start with a dataset to train on. Download the tiny shakespeare dataset\n",
    "!curl -0 https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501b9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3d2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of document is 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"Lenght of document is {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12bced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01867452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of bigrams found:  1403\n",
      "Unique characters:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab list size:  65\n"
     ]
    }
   ],
   "source": [
    "# Get an ordonned list based on the set of unique characters in the dataset\n",
    "bigrams = sorted(list(set([text[i:i+2] for i in range(len(text)-1)])))\n",
    "print(\"Amount of bigrams found: \", len(bigrams))\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_list = len(chars)\n",
    "print(\"Unique characters: \", ''.join(chars))\n",
    "print(\"Vocab list size: \", vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf431b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 1, 2]\n",
      "Hello there !\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the list of raw characters into a list of integers\n",
    "# Here we do a very basic tokenization, we just map each character to an integer\n",
    "# In practice, sub-word tokenization is often used (e.g. BPE, WordPiece, Unigram)\n",
    "# This is a more complex process that allows to handle out-of-vocabulary words and reduce the vocabulary size\n",
    "# When the vocabulary size is too large, the model can have difficulty learning the relationships between characters\n",
    "# and the training time can increase significantly. Sub-word tokenization helps to mitigate this issue.\n",
    "string_to_index_map = { ch:i for i,ch in enumerate(chars) } # string to index\n",
    "index_to_char_map = { i:ch for i,ch in enumerate(chars) } # index to string\n",
    "\n",
    "encode = lambda s: [string_to_index_map[c] for c in s] # encode a string to a list of integers\n",
    "decode = lambda l: ''.join([index_to_char_map[i] for i in l]) # decode a list of integers to a string\n",
    "\n",
    "print (encode(\"Hello there !\"))\n",
    "print (decode(encode(\"Hello there !\")))\n",
    "\n",
    "# Bigrams alternative\n",
    "# string_to_index_map_bigrams = { bg:i for i, bg in enumerate(bigrams)}\n",
    "# index_to_bigram_map = { i:bg for i, bg in enumerate(bigrams)}\n",
    "\n",
    "encode_bigrams = lambda s: [string_to_index_map_bigrams[s[i:i+2]] for i in range(len(s)-1)] # encode a string to a list of integers\n",
    "decode_bigrams = lambda l: ''.join([index_to_bigram_map[i] for i in l]) # decode a list of integers to a string\n",
    "\n",
    "# Error because the text does not contins ' !' So it cannot translate that part.\n",
    "# print(encode_bigrams(\"Hello there !\"))\n",
    "# print(decode_bigrams(encode_bigrams(\"Hello there !\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b212193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) - torch.int64 - torch.LongTensor\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Now we encode the entire text and split it into training and validation sets\n",
    "# For this we will use the Torch library to create a dataset and a dataloader\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # encode the entire text then wrap it in a torch.tensor\n",
    "\n",
    "print(\"{} - {} - {}\".format(data.shape, data.dtype, data.type())) # shape of the data tensor and its type\n",
    "print(data[:200]) # Print the characters that we looked at before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444b8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into training and validation sets\n",
    "# Validation part is kept for the end of the training to evaluate the model performance\n",
    "n = int(0.9 * len(data)) # 90% of the data for training and 10% for validation\n",
    "train_data = data[:n] # training data\n",
    "val_data = data[n:] # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e0349a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You never enter all of the dataset at once in your model, that would be physically impossible (in theory, the dataset is way bigger than ours)\n",
    "block_size = 8 # number of characters to feed to the model at once\n",
    "train_data[:block_size+1] # For each blocks, 8 individual exemple will be remembered by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a91443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "# Time dimension : In this small block, there are 8 rules of \"following characters\"\n",
    "input = train_data[:block_size] # input is the first block_size characters\n",
    "next = train_data[1:block_size+1] # target is the next character in the sequence\n",
    "\n",
    "# The training is done that way so the model learns to predict the next characters no matter the context rather than only the last one\n",
    "for t in range(block_size):\n",
    "    context = input[:t+1]\n",
    "    target = next[t]\n",
    "    print(f\"when input is {(context)}, target is {(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5acc18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input:  torch.Size([4, 8]) \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Batch target:  torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is tensor([24]), target is 43\n",
      "when input is tensor([24, 43]), target is 58\n",
      "when input is tensor([24, 43, 58]), target is 5\n",
      "when input is tensor([24, 43, 58,  5]), target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), target is 39\n",
      "when input is tensor([44]), target is 53\n",
      "when input is tensor([44, 53]), target is 56\n",
      "when input is tensor([44, 53, 56]), target is 1\n",
      "when input is tensor([44, 53, 56,  1]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52]), target is 58\n",
      "when input is tensor([52, 58]), target is 1\n",
      "when input is tensor([52, 58,  1]), target is 58\n",
      "when input is tensor([52, 58,  1, 58]), target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), target is 46\n",
      "when input is tensor([25]), target is 17\n",
      "when input is tensor([25, 17]), target is 27\n",
      "when input is tensor([25, 17, 27]), target is 10\n",
      "when input is tensor([25, 17, 27, 10]), target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), target is 39\n"
     ]
    }
   ],
   "source": [
    "# Batch dimension : Every time we train the model, we will use a batch of data to train it. This is done to speed up the training process and to make it more stable.\n",
    "torch.manual_seed(1337) # set the seed for reproducibility\n",
    "batch_size = 4 # number of independent sequences to process in parallel\n",
    "block_size = 8 # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of input and target to feed the model\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random starting index for each sequence in the batch. Batck_size number of blocks will be created\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # input data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # target data (what comes after should be guessed)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Batch input: \", xb.shape, \"\\n\", xb) # shape of the input batch\n",
    "print(\"Batch target: \", yb.shape, \"\\n\", yb) # shape of the target batch\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {(context)}, target is {(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa499bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Print the first input batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1956e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# Here, we use the simpliest neural network : bigrams\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) # set the seed for reproducibility\n",
    "\n",
    "# We predict what happen next based on only a single token\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    # We create a token embedding table of size vocab_size x vocab_size\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token via a lookup table\n",
    "        # This is a simple model that uses a token embedding table to map each character to a vector of size vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # Each \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channels) 4, 8 , 65\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        # but pytorch expect the dimension in a different order so we need to reshape our logic\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (Batch * Time, Channels) 32, 65 : 2 dimensionnal array\n",
    "            targets = targets.view(B*T) # (Batch * Time) 32 : 1 dimensionnal array\n",
    "            loss = F.cross_entropy(logits, targets)# To mesure the loss / negative prediction\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # This method take a (B, T) array of indices and generates new tokens based on the current context limited by max_new_tokens\n",
    "    # idx is the current context, max_new_tokens is the number of new tokens to generate\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions : This is where you would target the loss function, in the current case, we don't use it\n",
    "            logits, loss = self(idx, None)\n",
    "            # focus on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C) where C is the vocab size\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_list) # create the model\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate 100 new tokens starting from the context of a single zero index (which is the first character in the vocab)\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long) # 1 by 1 tensor where the d.type is integer\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) \n",
    "# The result is bad because this is random generated model. It doesn't use the history, only the last character of the input to generate the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717648e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer to optimize the model parameters\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # AdamW optimizer with a learning rate of 1e-3. Smaller model can go with faster learning rate, bigger model need smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e0ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# Typical training loop\n",
    "for steps in range(10000): # the more loops, the more the model will learn\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, xy = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss (need to be low but not so low that it give an exact copy of the training data)\n",
    "    logits, loss = m(xb, xy)\n",
    "    optimizer.zero_grad(set_to_none=True) # set the gradients to zero before the backward pass\n",
    "    loss.backward() # Backward pass to compute the gradients\n",
    "    optimizer.step() # Update the model parameters using the gradients\n",
    "\n",
    "print(loss.item()) # Print the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900415a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "# now, we can generate new text using the trained model. This wont be perfect because the token do not speak to each other\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e382780",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfe75ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337) # set the seed for reproducibility\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B, T, C) # random input tensor\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b74914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want token to get information about their past, about the previous tokens (not the future ones)\n",
    "# one very basic (which loses a lot of data) way to do this is to do an averge (or a sum) of the previous tokens\n",
    "# this is called a \"cumulative sum\" or \"running sum\"\n",
    "xbow = torch.zeros((B, T, C)) # x \"bag of words\", a tensor to store the cumulative sum\n",
    "for b in range(B): # batch dimension\n",
    "    for t in range(T): # time dimension\n",
    "        xprev = x[b,:t+1] # (t,C) => all previous tokens up to time t for batch b\n",
    "        # we take the mean of all previous tokens (including the current one) to get the cumulative sum\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "x[0]\n",
    "xbow[0] # the first token is the same as the input, the second token is the average of the first two tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f60a3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous code is very convenient since it get us the average of the previous tokens in a single line of code.\n",
    "# So lets vectorize it\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize the rows to get the average\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) => (B, T, C)\n",
    "torch.allclose(xbow, xbow2) # check if the two tensors are equal\n",
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6adc2b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3 : use softmax. \n",
    "tril = torch.tril(torch.ones(T,T)) # lower triangular 1 matrix\n",
    "# More interesting because the weights start at 0. \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # any 0 becomes -inf so that token from the past cannot communicate with the future ones, so we mask the upper triangular part of the matrix\n",
    "wei = F.softmax(wei, dim=-1) # apply softmax to get the weights\n",
    "xbow3 = wei @ x # (B, T, T) @ (B, T, C) => (B, T, C)\n",
    "xbow[0], xbow3[0] # check if the two tensors are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db9c35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3)) # get the lower triangular part of a matrix (including the diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf6c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# this was very slow because of the for loops.\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float() # randopm numbers between 0 and 10, 3 rows, 2 columns\n",
    "c = a @ b # matrix multiplication\n",
    "print(f\"a={a}\\nb={b}\\nc={c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c779e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # normalize the rows to get the average\n",
    "b = torch.randint(0,10,(3,2)).float() # randopm numbers between 0 and 10, 3 rows, 2 columns\n",
    "c = a @ b # matrix multiplication\n",
    "print(f\"a={a}\\n--\\nb={b}\\n--\\nc={c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d767e78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention !\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # key projection\n",
    "query = nn.Linear(C, head_size, bias=False) # query projection\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# No communication yet, just creation of keys\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) => (B, T, T) For every row of B we get a matrix of size (T, T) where each row is the dot product of the query and the key\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "# for the purpose of a single head : Here what I am interested in - Here what I have - If you find me interesting, here what I will communicate to you\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151154af",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a __communication mechanism__. Can be seen as nodes in a directed graph looking at each other and aggregating informations with a weighted sum from all nodes that point to them with data-dependent weights.\n",
    "- There is no notion of space. Attention simply act over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example accross batch dimensions is of course processed completly independently and never \"talk\" to each other.\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This here is called a \"decoder\" attention block because it has triangular masking and is usually used in autoregressive settings, like ... modeling. ( you would remove the line wei = wei.masked_fill(tril = 0, float('-inf')) which masks the future token from the current one)\n",
    "- \"Self-attention\" is called such because all the value are coming from the same source (x) but in principal, attention can be more general. \"Cross-attention\" would be if we want a separate pool of nodes from which we want to pool information onto our node.\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). this makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Since wei feeds into softmax so it is important for it to be fairly defused (especially during initialization) otherwise softmax will converge toward a very strong vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bf21650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer normalization is a technique to normalize the activations of a layer in a neural network.\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # mean over the batch dimension\n",
    "        xvar = x.var(1, keepdim=True, unbiased=False)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize the unit variance\n",
    "        self.out = self.gamma * xhat + self.beta # scale and shift\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)\n",
    "out = module(x) # forward pass\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
